---
title: "Final Exam"
author: "Dawid DÄ…bkowski"
date: "2 February 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PRE-PROCESSING

We start by setting the random seed and loading useful libraries.

```{r, message=F, warning=F}
set.seed(02022018)
require(RCurl)
require(gdata)
library(dplyr)
library(magrittr)
library(ggplot2)
library(MASS)
#library(MVN) #multivariate normality test
library(caret)
library(lars)
```

Now we load our data frames for the exercises.

```{r}
urls <- c("https://www.mimuw.edu.pl/~noble/courses/SDA/data/ushighways.txt",
         "https://www.mimuw.edu.pl/~noble/courses/SDA/data/PET.txt",
         "https://www.mimuw.edu.pl/~noble/courses/SDA/data/ozone.csv",
         "https://www.mimuw.edu.pl/~noble/courses/SDA/data/pendigits.txt",
         "https://www.mimuw.edu.pl/~noble/courses/SDA/data/carmarks.txt",
         "https://www.mimuw.edu.pl/~noble/courses/SDA/data/primate.scapulae.txt")
ushighways <- read.table(urls[1], header=T)
bodyfat <- read.xls("bodyfat2.xlsx")
yarn <- read.table(urls[2], header=T)   #data(yarn, package="pls")
ozone <- read.csv(urls[3])
pendigits <- read.table(urls[4])
carmarks <- read.table(urls[5], sep=";", header=T)
scapular <- read.table(urls[6], sep=" ", header=T)
```

## EXERCISE 1

Let us take a look at data.

```{r}
head(ushighways)
```

We first draw some histograms with different window widths.

```{r}
ggplot(ushighways, aes(x=Approx.Miles)) + 
    geom_histogram(size=2, alpha=0.05, binwidth=20, boundary=0, 
                   aes(color="20", y=..density..)) + 
    geom_histogram(size=1.5, alpha=0.05, binwidth=10, boundary=0, 
                   aes(color="10", y=..density..)) +
    geom_histogram(size=1, alpha=0.05, binwidth=5, boundary=0, 
                   aes(color="5", y=..density..)) +
    theme_bw() + theme(plot.title = element_text(hjust = 0.5)) + ggtitle("Histograms") +
    guides(color=guide_legend(title="Window width")) + 
    theme(legend.position=c(0.9,0.8), legend.background=element_rect(fill=alpha('blue',0)))
```

The second picture with window width of 10 miles looks quite good. It is well-splitted and shows the  monotonicity of the distribution.

Now we will use UCV, BCV and SJPI estimators for window width to plot densities with gaussian kernel.

```{r}
sjpi <- width.SJ(ushighways$Approx.Miles, method="dpi")
ggplot(ushighways, aes(x=Approx.Miles)) + 
    geom_density(fill="grey", size=0.8, alpha=0.05, bw="ucv", aes(color="UCV")) + 
    geom_density(fill="grey", size=0.8, alpha=0.05, bw="bcv", aes(color="BCV")) +
    geom_density(fill="grey", size=0.8, alpha=0.05, bw="sj", aes(color="SJ")) +
    geom_density(fill="grey", size=0.8, alpha=0.05, bw=sjpi, aes(color="SJPI")) +
    theme_bw() + theme(plot.title = element_text(hjust = 0.5)) + 
    ggtitle("Density estimators") + guides(color=guide_legend(title="Window width")) + 
    theme(legend.position=c(0.9,0.8), legend.background=element_rect(fill=alpha('blue',0)))
```

The BCV estimator (red one) looks quite well-fitted and monotonic. The other kernel choices (triangular, cosine etc.) gave very similar results.

## EXERCISE 2

Let us take a look at data.

```{r}
head(bodyfat[,1:6])
```

We now calculate correlation matrix for 13 explanatory variables.

```{r}
round(cor(bodyfat[,-(1:2)]), 2)
```

Some of the variables are highly correlated (around 80-90%), which may lead to ill-conditioning for regression. Let us fit a regression model.

```{r}
bodyfat_fit <- lm(bodyfat~., bodyfat[,-1])
summary(bodyfat_fit)
```

The signifficant variables are: age, neck, abdomen, forearm and wrist. Let us now fit regression by stepwise elimination.

```{r}
bodyfat_fit_forward <- step(lm(bodyfat~1, bodyfat[,-1]), direction="forward",
                            scope=formula(bodyfat_fit), trace=0)
summary(bodyfat_fit_forward)$call
bodyfat_fit_backward <- step(lm(bodyfat~., bodyfat[,-1]), direction="backward", trace=0)
summary(bodyfat_fit_backward)$call
```

Both on the forward and backward stepwise elimination yield the same model based on 8 variables. Now we wan't to find the best model using leave-one-out cross-validation. It is computationally hard to check all possible models of 13 predictors (approx time 4h). We will then implement and use backward stepwise elimination heuristics instead.

```{r, cache=T, }
stepwise_loocv <- function(variables, trace=F){
    bodyfat_ctrl <- trainControl(method="LOOCV")
    len <- length(variables)
    errors <- data.frame(var=1:len, err=rep(NA,len))
    init_form <- as.formula(paste("bodyfat~", paste(variables, collapse="+")))
    init_fit <- train(init_form, bodyfat[,-1], method="lm", trControl=bodyfat_ctrl)
    init_err <- init_fit$results$RMSE
    for (i in 1:len){
        form <- as.formula(paste("bodyfat~", paste(variables[-i], collapse="+")))
        fit <- train(form, bodyfat[,-1], method="lm", trControl=bodyfat_ctrl)
        errors$err[i] <- fit$results$RMSE
    }
    worst_var <- which.min(errors$err)
    if (trace) print(paste0("bodyfat~", paste(substr(variables, 1, 4), collapse="+"), 
                            ": ", round(init_err,2)))
    if (errors$err[worst_var]<init_err) stepwise_loocv(variables[-worst_var], trace=T)
    else return(init_fit$finalModel)
}

bodyfat_fit_loocv <- stepwise_loocv(colnames(bodyfat[,-(1:2)]), trace=T)
formula(bodyfat_fit_loocv)
```

This method yields the same model as the ones from the elimination based on AIC. Now we fit a LASSO model and look for a best fit by a leave-one-out cross-validation.

```{r, fig.height=4}
bodyfat_fit_lasso <- lars(as.matrix(bodyfat[,-(1:2)]), bodyfat[,2], type="lasso")
plot(bodyfat_fit_lasso)
bodyfat_lasso <- cv.lars(as.matrix(bodyfat[,-(1:2)]), bodyfat[,2], K=252, type="lasso")
```

From this we estimate that the lowest MSE of `r which.min(bodyfat_lasso$cv)` is obtained for LASSO model with parameter `r bodyfat_lasso$index[which.min(bodyfat_lasso$cv)]`. Now we do the same analysis for LARS model.

```{r, fig.height=4}
bodyfat_fit_lars <- lars(as.matrix(bodyfat[,-(1:2)]), bodyfat[,2], type="lar")
plot(bodyfat_fit_lars)
bodyfat_lars <- cv.lars(as.matrix(bodyfat[,-(1:2)]), bodyfat[,2], K=252, type="lar")
```

From this we estimate that the lowest MSE of `r which.min(bodyfat_lasso$cv)` is obtained for LARS model with parameter `r bodyfat_lasso$index[which.min(bodyfat_lasso$cv)]`. This is a much better result comparing to LASSO model.


## EXERCISE 3

Let us take a look at data.

```{r}
head(yarn[,1:6])
```


## EXERCISE 4

Let us take a look at data.

```{r}
head(ozone)
```

## EXERCISE 5

Let us take a look at data.

```{r}
head(pendigits[,1:6])
```

## EXERCISE 6

Let us take a look at data.

```{r}
head(carmarks[,1:6])
```

## EXERCISE 7

Let us take a look at data.

```{r}
head(scapular[,1:6])
```

## EXERCISE 8

Let us take a look at data.

```{r}
head(scapular[,1:6])
```