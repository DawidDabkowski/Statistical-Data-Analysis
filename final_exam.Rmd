---
title: "Final Exam"
author: "Dawid DÄ…bkowski"
date: "2 February 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## PRE-PROCESSING

We start by setting the random seed and loading useful libraries.

```{r, message=F, warning=F}
set.seed(02022018)
library(RCurl)
library(gdata)
library(dplyr)
library(magrittr)
library(ggplot2)
library(MASS)
library(caret)
library(lars)
library(pls)
library(glmnet)
library(ade4)
library(CCA)
library(yacca)
library(cluster)
```

Now we load our data frames for the exercises.

```{r}
urls <- c("https://www.mimuw.edu.pl/~noble/courses/SDA/data/ushighways.txt",
         "https://www.mimuw.edu.pl/~noble/courses/SDA/data/PET.txt",
         "https://www.mimuw.edu.pl/~noble/courses/SDA/data/ozone.csv",
         "https://www.mimuw.edu.pl/~noble/courses/SDA/data/pendigits.txt",
         "https://www.mimuw.edu.pl/~noble/courses/SDA/data/carmarks.txt",
         "https://www.mimuw.edu.pl/~noble/courses/SDA/data/primate.scapulae.txt")
ushighways <- read.table(urls[1], header=T)
bodyfat <- read.xls("bodyfat2.xlsx")
yarn <- read.table(urls[2], header=T)   #data(yarn, package="pls")
ozone <- read.csv(urls[3])
pendigits <- read.table(urls[4])
carmarks <- read.table(urls[5], sep=";", header=T)
scapular <- read.table(urls[6], sep=" ", header=T)
```

## EXERCISE 1

Let us take a look at data.

```{r}
head(ushighways)
```

We first draw some histograms with different window widths.

```{r}
ggplot(ushighways, aes(x=Approx.Miles)) + 
    geom_histogram(size=2, alpha=0.05, binwidth=20, boundary=0, 
                   aes(color="20", y=..density..)) + 
    geom_histogram(size=1.5, alpha=0.05, binwidth=10, boundary=0, 
                   aes(color="10", y=..density..)) +
    geom_histogram(size=1, alpha=0.05, binwidth=5, boundary=0, 
                   aes(color="5", y=..density..)) +
    theme_bw() + theme(plot.title = element_text(hjust = 0.5)) + ggtitle("Histograms") +
    guides(color=guide_legend(title="Window width")) + 
    theme(legend.position=c(0.9,0.8), legend.background=element_rect(fill=alpha('blue',0)))
```

The second picture with the window width of 10 miles looks quite good. It is well-divided and shows the monotonicity of the distribution.

Now we will use UCV, BCV and SJPI estimators for window width to plot densities with the gaussian kernel.

```{r}
sjpi <- width.SJ(ushighways$Approx.Miles, method="dpi")
ggplot(ushighways, aes(x=Approx.Miles)) + 
    geom_density(fill="grey", size=0.8, alpha=0.05, bw="ucv", aes(color="UCV")) + 
    geom_density(fill="grey", size=0.8, alpha=0.05, bw="bcv", aes(color="BCV")) +
    geom_density(fill="grey", size=0.8, alpha=0.05, bw="sj", aes(color="SJ")) +
    geom_density(fill="grey", size=0.8, alpha=0.05, bw=sjpi, aes(color="SJPI")) +
    theme_bw() + theme(plot.title = element_text(hjust = 0.5)) + 
    ggtitle("Density estimators") + guides(color=guide_legend(title="Window width")) + 
    theme(legend.position=c(0.9,0.8), legend.background=element_rect(fill=alpha('blue',0)))
```

The BCV estimator (the red one) looks quite well-fitted and monotonic. The other kernel choices (triangular, cosine etc.) gave very similar results.

## EXERCISE 2

Let us take a look at data.

```{r}
head(bodyfat[,1:6])
```

We now calculate the correlation matrix for 13 explanatory variables.

```{r}
cor_matrix <- round(cor(bodyfat[,-(1:2)]), 2)
row.names(cor_matrix) %<>% sapply(function(x) substr(x, 1, 1))
colnames(cor_matrix) %<>% sapply(function(x) substr(x, 1, 4))
cor_matrix
```

Some of the variables are highly correlated (around 80-90%), which may lead to ill-conditioning for regression. Let us fit a regression model.

```{r}
bodyfat_fit <- lm(bodyfat~., bodyfat[,-1])
summary(bodyfat_fit)
```

The significant variables are age, neck, abdomen, forearm, and wrist. Let us now fit regression by stepwise elimination.

```{r}
bodyfat_fit_forward <- step(lm(bodyfat~1, bodyfat[,-1]), direction="forward",
                            scope=formula(bodyfat_fit), trace=0)
summary(bodyfat_fit_forward)$call
bodyfat_fit_backward <- step(lm(bodyfat~., bodyfat[,-1]), direction="backward", trace=0)
summary(bodyfat_fit_backward)$call
```

Both on the forward and backward stepwise elimination yield the same model based on 8 variables. Now we want to find the best model using leave-one-out cross-validation. It is computationally hard to check all possible models of 13 predictors (approx time 4h). We will then implement and use backward stepwise elimination heuristics instead.

```{r, cache=T, }
stepwise_loocv <- function(variables, trace=F){
    bodyfat_ctrl <- trainControl(method="LOOCV")
    len <- length(variables)
    errors <- data.frame(var=1:len, err=rep(NA,len))
    init_form <- as.formula(paste("bodyfat~", paste(variables, collapse="+")))
    init_fit <- train(init_form, bodyfat[,-1], method="lm", trControl=bodyfat_ctrl)
    init_err <- init_fit$results$RMSE
    for (i in 1:len){
        form <- as.formula(paste("bodyfat~", paste(variables[-i], collapse="+")))
        fit <- train(form, bodyfat[,-1], method="lm", trControl=bodyfat_ctrl)
        errors$err[i] <- fit$results$RMSE
    }
    worst_var <- which.min(errors$err)
    if (trace) print(paste0("bodyfat~", paste(substr(variables, 1, 4), collapse="+"), 
                            ": ", round(init_err,2)))
    if (errors$err[worst_var]<init_err) stepwise_loocv(variables[-worst_var], trace=T)
    else return(init_fit$finalModel)
}

bodyfat_fit_loocv <- stepwise_loocv(colnames(bodyfat[,-(1:2)]), trace=T)
formula(bodyfat_fit_loocv)
```

This method yields the same model as the ones from the elimination based on AIC. Now we fit a LASSO model and look for the best fit by a leave-one-out cross-validation.

```{r, fig.height=4}
bodyfat_fit_lasso <- lars(as.matrix(bodyfat[,-(1:2)]), bodyfat[,2], type="lasso")
plot(bodyfat_fit_lasso)
bodyfat_lasso <- cv.lars(as.matrix(bodyfat[,-(1:2)]), bodyfat[,2], K=252, type="lasso")
```

From this we estimate that the lowest MSE of `r which.min(bodyfat_lasso$cv)` is obtained for LASSO model with parameter `r bodyfat_lasso$index[which.min(bodyfat_lasso$cv)]`. Now we do the same analysis for LARS model.

```{r, fig.height=4}
bodyfat_fit_lars <- lars(as.matrix(bodyfat[,-(1:2)]), bodyfat[,2], type="lar")
plot(bodyfat_fit_lars)
bodyfat_lars <- cv.lars(as.matrix(bodyfat[,-(1:2)]), bodyfat[,2], K=252, type="lar")
```

From this we estimate that the lowest MSE of `r which.min(bodyfat_lasso$cv)` is obtained for LARS model with parameter `r bodyfat_lasso$index[which.min(bodyfat_lasso$cv)]`. This is a much better result comparing to LASSO model.


## EXERCISE 3

Let us take a look at data. We will separate the test set to use it for performance test later.

```{r}
head(yarn[,1:6])
yarn_train <- (yarn %>% filter(train==1))[,-270]
yarn_test <- (yarn %>% filter(train==0))[,-270]
```

Now we perform PCR and PSLR. We will estimate the error for a different number of components by the leave-one-out cross-validation.

```{r}
yarn_fit_pcr <- pcr(y~., ncomp=20, data=yarn_train, validation="LOO")
summary(yarn_fit_pcr)
plot(RMSEP(yarn_fit_pcr), legendpos="topright")
```

We get the lowest cross-validated error for 19 components, so we choose this number. By the look of the graph we could take fewer components for simplicity (not much difference after 6-th component).

```{r}
yarn_fit_plsr <- plsr(y~., ncomp=20, data=yarn_train, validation="LOO")
summary(yarn_fit_plsr)
plot(RMSEP(yarn_fit_plsr), legendpos="topright")
```

We get the lowest cross-validated error for 14 components, so we choose this number. By the look of the graph we could take fewer components for simplicity (not much difference after 5-th component).

Now we perform a ridge regression with cross-validation.

```{r}
yarn_fit_ridge <- cv.glmnet(as.matrix(yarn_train[,-269]), yarn_train[,269], nfolds=21)
```

We choose the model with lambda.1se estimator, which equals to `r yarn_fit_ridge$lambda.1se`.

Now we compare all models.

```{r}
predict(yarn_fit_pcr, newdata=yarn_test)[,,19] -> a
RMSEP(yarn_fit_pcr, newdata=yarn_test)$val[19]
RMSEP(yarn_fit_plsr, newdata=yarn_test)$val[14]
sqrt(mean((predict(yarn_fit_ridge, newx=as.matrix(yarn_test[,-269]),
                   s=yarn_fit_ridge$lambda.1se)-yarn_test[,269])^2))
```

We can see that the PCR and PLSR models perform very well. The ridge model gives slightly worse results for this example. More precise lambda search might be helpful.

## EXERCISE 4

Let us take a look at data.

```{r}
head(ozone)
```

To perform Mantel test, we need to calculate two matrices - one with measurement differences and one with geographic distances.

```{r}
ozone_m1 <- dist(ozone$Av8top)
ozone_m2 <- dist(cbind(ozone$Lon, ozone$Lat))
set.seed(06022018)
mantel.rtest(ozone_m1, ozone_m2)
```

The simulated p-value is lower than 0.05, which suggest that the alternative hypothesis of related distances is true. We can say that stations, which are closer, have similar ozone measurements.

## EXERCISE 5

Let us take a look at data.

```{r}
head(pendigits[,1:6])
```

Some of the columns are redundant. We will throw them away.

```{r}
pendigits <- pendigits[,1:17]
```

We start by computing the variances.

```{r}
sapply(pendigits[,-17], var)
```

We see that variables differ. Now we perform a PCA on the covariance matrix.

```{r}
pendigits_pcacov <- prcomp(pendigits[,-17], scale=F)
summary(pendigits_pcacov)
```

As we can see, taking just 5 components retain more than 80% of data variance; taking 7 retain more than 90%. Now we visualize three first components on the scatterplots.

```{r}
plot(data.frame(pendigits_pcacov$x[,1:3]))
```

We will now carry a PCA with the correlation matrix.

```{r}
pendigits_pcacor <- prcomp(pendigits[,-17], scale=T)
summary(pendigits_pcacor)
plot(data.frame(pendigits_pcacor$x[,1:3]))
```

The scaled PCA performs slightly worse on this data (we need one more component to explain 80% or 90% of variance). Some of the components are more informative and some are less. Now we make scree plots for both approaches.

```{r}
cov<-data.frame(ind=1:16, val=summary(pendigits_pcacov)$importance[3,], type=rep("cov",16))
cov<-rbind(list(0,0,"cov"), cov)
cor<-data.frame(ind=1:16, val=summary(pendigits_pcacor)$importance[3,], type=rep("cor",16))
cor<-rbind(list(0,0,"cor"), cor)
ggplot(rbind(cov,cor), aes(x=ind, y=val, color=type)) + geom_line(size=1.2, alpha=0.7) + 
     theme_bw() + theme(legend.position=c(0.9,0.5)) + geom_point()
```

We can see that except the first component, the covariance matrix gave better principal components in terms of explained variance. Using elbow method we would take 10 components for further analysis. If we had to reduce dimension even further, we would take 4-5 components.

From the PCA we can see that there is some ill-conditioning of the data matrix - the fractions of variance added from each component are not equal. By taking simply 2 first components, we get more than half information (average of 8 initial variables).

## EXERCISE 6

Let us take a look at data.

```{r}
head(carmarks)
```

We divide the data into X and Y matrices and check the correlations inside these groups. and perform a CCA.

```{r}
carmarks_X <- carmarks[,4:5]
carmarks_Y <- carmarks[,c(2:3,6:9)]
round(cor(carmarks_X),2)
round(cor(carmarks_Y),2)
```

Some of the variables are highly correlated, which may cause problems in CCA. Nevertheless, we will perform CCA and interpret results.

```{r}
carmarks_cca <- cca(carmarks_X, carmarks_Y, 
                    xcenter=T, ycenter=T, xscale=T, yscale=T, standardize.score=T)
```

Before we move into conclusions, we will perform a chi-squared test for the significance of canonical correlations.

```{r}
pchisq(carmarks_cca$chisq, carmarks_cca$df, ncp=0)
```

There is no basis to reject the null hypothesis, so we can assume that canonical correlations are significant. Now let us take a look at the cca object.

```{r}
carmarks_cca
```

As the smaller group has 2 variables, we have two canonical variables CV1 and CV2. We can see that the highest correlation (0.979) is between PRICE-VALUE and ECONOMY-SERVICE-SPORT-SAFETY-EASYINESS (with some large coefficients). So the higher price and lower value stability, the more economy and less service, sporty, safety and easy to use the car is. The second canonical variable with correlation 0.885 is between -VALUE-PRICE and -ECONOMY-SERVICE-EASINESS (with some large coefficients). So the larger is the value stability and price, the more economy, service and easy to use car is.

If we look at the correlations instead of coefficients, the numbers differ. For example, the first variable is VALUE-PRICE but correlated with ECONOMY-SERVICE-DESIGN-SPORT-SAFETY (design instead of easyiness) and with different coefficients.

This is the main result and it should be considered that the input correlation matrices were quite ill-conditioned. It gives some intuition about the main probable dependencies between X and Y groups of variables. 

## EXERCISE 7

Let us take a look at data.

```{r}
head(scapular)
```

Before we start cluster analysis, we scale the columns, so the euclidean distance will have a better interpretation.

```{r}
scapular <- scapular[,-9]
scapular[,2:8] %<>% scale
```

Now we cluster our data using kmeans, pam (divisive) and single, average, complete and ward linkage (hierarchical). For devisive methods we use k=5. For hierarchical we plot dendrograms and cut them to get 5 clusters as well.

```{r, fig.height=4}
scapular_kmeans <- kmeans(scapular[,2:8], centers=5)
table(scapular_kmeans$cluster)

scapular_pam <- pam(scapular[,2:8], k=5)
table(scapular_pam$cluster)

scapular_dist <- dist(scapular[,2:8], method="euclidean")

scapular_single <- hclust(scapular_dist, method="single")
scapular_average <- hclust(scapular_dist, method="average") 
scapular_complete <- hclust(scapular_dist, method="complete") 
scapular_ward <- hclust(scapular_dist, method="ward") 

plot(scapular_single)
rect.hclust(scapular_single, k=5, border="red")
scapular_single %<>% cutree(k=5)

plot(scapular_average)
rect.hclust(scapular_average, k=5, border="red")
scapular_average %<>% cutree(k=5)

plot(scapular_complete)
rect.hclust(scapular_complete, k=5, border="red")
scapular_complete %<>% cutree(k=5)

plot(scapular_ward)
rect.hclust(scapular_ward, k=5, border="red")
scapular_ward %<>% cutree(k=5)
```

From the summaries and pictures, we can see that devisive algorithms don't give outliers. From the hierarchical methods, single and average linkage produce outliers, while complete and ward linkage does not. Now we compare the results with theoretical groups and write confusion matrices.

```{r}
confusionMatrix(scapular_kmeans$cluster, scapular$classdigit)$table
confusionMatrix(scapular_kmeans$cluster, scapular$classdigit)$overall[1]

confusionMatrix(scapular_pam$cluster, scapular$classdigit)$table
confusionMatrix(scapular_pam$cluster, scapular$classdigit)$overall[1]

confusionMatrix(scapular_single, scapular$classdigit)$table
confusionMatrix(scapular_single, scapular$classdigit)$overall[1]

confusionMatrix(scapular_average, scapular$classdigit)$table
confusionMatrix(scapular_average, scapular$classdigit)$overall[1]

confusionMatrix(scapular_complete, scapular$classdigit)$table
confusionMatrix(scapular_complete, scapular$classdigit)$overall[1]

confusionMatrix(scapular_ward, scapular$classdigit)$table
confusionMatrix(scapular_ward, scapular$classdigit)$overall[1]
```

The devisive methods gave quite good results: 0.55 acc for kmeans and 0.67 acc for pam. The hierarchical methods with single and average linkage are worse on this dataset (0.29 and 0.28 acc) but complete and ward linkage works well (0.62 and 0.70 acc).

## EXERCISE 8

Let us take a look at data.

```{r}
scapular <- read.table(urls[6], sep=" ", header=T)
head(scapular)
```

```{r, eval=F, include=F}
library(MVN)

for (i in 1:5){
    hzTest(scapular[scapular$classdigit==i, 2:8], qqplot=F)
    mardiaTest(scapular[scapular$classdigit==i, 2:8], qqplot=F)
    roystonTest(scapular[scapular$classdigit==i, 2:8], qqplot=F)
}
```

We have checked that the data points inside particular classes are generally multivariate normal, which satisfies the assumptions for lda. We peform a leave-one-out cross-validated lda for each reference primates vs others and check, which individuals were classified to certain regions.

```{r}
ambireg <- 1:105
primates <- c("Gorilla", "Homo", "Hylobates", "Pan", "Pongo")
for (prim in primates) {
    df <- scapular
    df$class %<>% sapply(function(x) ifelse(x==prim, prim, "other"))
    df$class %<>% factor
    fit <- lda(class~., data=df[,c(2:8,10)], CV=T)
    specreg <- which(fit$class==prim)
    len <- min(length(specreg), 20)
    text <- paste(prim, "region:", paste(specreg[1:len], collapse=" "))
    if (length(specreg)>20) text <- paste(text, "[...]")
    print(text)
    ambireg <- setdiff(ambireg, specreg)
}

print(paste("Ambiguous region:", paste(ambireg, collapse=" ")))
```

Now we perform 10 lda for each pair of primates and predict the whole data. We then classify the species by the majority vote.

```{r}
pairs <- combn(primates, 2)
preds <- data.frame(matrix(NA, 105, 10))
for (i in 1:10) {
    pair <- pairs[,i]
    df <- scapular %>% filter(class %in% pair)
    df$class %<>% drop.levels()
    fit <- lda(class~., data=df[,c(2:8,10)])
    preds[,i] <- predict(fit, scapular[,c(2:8)])$class
}
vote <- function(x) names(sort(table(t(x)), decreasing=T)[1])
scapular_pred <- apply(preds, 1, vote)
confusionMatrix(scapular_pred, scapular$class)$table
```

We have obtained the almost perfect fit to the training data with 99% accuracy (only 1 misclasification). Let us now compare it with the straight use of lda with 5 boundaries.

```{r}
scapular_fit <- lda(class~., data=scapular[,c(2:8,10)], CV=T)
confusionMatrix(scapular_fit$class, scapular$class)$table
```

We can see that this classifier is slightly worse on training data with 97% accuracy (3 misclasifications). It seems like the voting method is better. To be sure, we would need some test data to check how it generalizes to new examples.